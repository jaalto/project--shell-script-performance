FILE: t-array-split-string.bash
#! /bin/bash
#
# Q: split string into array: read vs eval?
# A: eval is 5x faster
#
# t1 real    0m0.025s read -ra
# t2 real    0m0.005s eval     (!)
#
# Code:
#
# string=$(echo {1..100})
# read -ra array <<< "$string"  # t1
# eval 'array=($string)'        # t2

FILE: t-call-function-and-return-value.bash
#! /bin/bash
#
# Q: Bash name ref to return values vs val=$(funcall)
# A: name ref is about 40x faster
#
# t1 real    0m0.089s t1 $(funcall)
# t2 real    0m0.002s t2 funcall nameref
#
# Code:
#
# fn(): return "echo <value>"     # t1
# fn(): return using local -n var # t2

FILE: t-command-grep.sh
#! /bin/bash
#
# Q: In grep, is --fixed-strings faster?
# A: Not much difference to --extended-regexp or --perl-regexp
#
# Q: Is using parallel(1) with grep even faster?
# A: Not worth for small files. Yes with bigger ones (test file: 10 000 lines)
#
# t1pure     real   0m0.338s LANG=C --fixed-strings
# t1utf8     real   0m0.372s LANG=C.UTF-8 --fixed-strings
# t1extended real   0m0.346s LANG=C --extended-regexp
# t1perl     real   0m0.349s LANG=C --perl-regexp
#
# t2icasef   real    0m0.394s LANG=C --fixed-strings --ignore-case
# t2icasee   real    0m0.419s LANG=C --extended-regexp --ignore-case
#
# GNU parallel(1). Split file into chunks and run grep(1) in parallel
# for each chunk.
#
# t_parallel1 real  0m0.226s <defaults>
# t_parallel2 real  0m0.653s --block-size 1k
# t_parallel3 real  0m0.300s -N 1k (grep instance for every 1k lines)

FILE: t-dir-empty.sh
#! /bin/bash
#
# Q: What is the fastest way to check empty directory?
# A: array+glob is faster than built-in compgen
#
# t1 real    0m0.054s   array+glob
# t2 real    0m0.104s   compgen
# t3 real    0m0.304s   ls (out of curiosity)
# t3 real    0m0.480s   find|read

FILE: t-dir-entries.sh
#! /bin/bash
#
# Q: Fastest to get list of dirs: for vs compgen vs ls -d
# A: In general, simple ls(1) will do fine. No big differences.
#
# for 20 directories:
#
# t3 real    0m0.003s compgen -G */
# t1 real    0m0.001s for-loop
# t2 real    0m0.004s ls -d */
#
# for 100 directories:
#
# t1 real    0m0.012s compgen -G */
# t2 real    0m0.015s for-loop
# t3 real    0m0.010s ls -d */
#
# notes
#
# Because the OS caches files and directories, you have to
# manually run tests:
#
#     max_dirs=20 ./t-dir-entries.sh t1
#     max_dirs=20 ./t-dir-entries.sh t2
#     max_dirs=20 ./t-dir-entries.sh t3

FILE: t-file-copy-check-exist.sh
#! /bin/bash
#
# Q: Need a copy of file. Call cp(1), make hardlink, or do test before copy?
# A: Faster is to test existense of file before cp(1). Hardlink is fast.
#
# t1 real    0m1.002s cp A B
# t2 real    0m0.013s test(1) -nt ...
# t2 real    0m0.009s test(1) -ef ... (using hardlink)
#
# Code:
#
# cp --preserve=timestamps A B                       # t1
# [ A -nt B ] || cp --preserve=timestamps ...        # t2
# [ A -ef B ] || cp --preserve=timestamps --link ... # t3

FILE: t-file-for-loop-vs-awk.sh
#! /bin/bash
#
# Q: for-loop to send file-by-file to awk vs awk handling all the files?
# A: pure awk is at least 2x faster
#
# t1 real    0m0.213s pure awk
# t1 real    0m0.584s for + awk
#
# Code:
#
# awk '{...}' <file...>
# for..do.. awk <file> .. done

FILE: t-file-glob-bash-compgen-vs-stat.sh
#! /bin/bash
#
# Q: The check if GLOB matches file: stat or Bash compgen?
# A: Bash array+glob/compgen are much faster than stat(1)
#
# t1 real    0m0.026s   Bash compgen GLOB
# t2 real    0m0.028s   Bash array: (GLOB)
# t2 real    0m0.039s   stat -t GLOB
#
# Code:
#
# arr=("file"*)
# compgen -G "file"*
# stat -t "file"*
#
# Notes:
#
# Understandable as stat(1) would do more work by
# opening each file found.

FILE: t-file-grep-vs-match-in-memory.sh
#! /bin/bash
#
# Q: To check file for matches: repeat read, inline match or grep(1)?
# A: Fastest is to read file once into memory and then match
#
# t1a real   0m0.049s read + bash regexp (read file once + use loop)
# t1b real   0m0.117s read + case..MATCH..esac (read file once + use loop)
# t2  real   0m0.482s read + case..MATCH..esac (separate file calls)
# t3  real   0m0.448s read + bash regexp (separate file calls)
# t4  real   0m0.404s external grep(1)
#
# Code:
#
# Study the <file>.sh for more information.
#
# read once and loop [[ str =~~ RE ]]   # t1a
# read once and loop case..MATCH..end   # t1b
# read -N<max> < file. case..MATCH..end # t2
# read -N<max> < file. [[ str =~~ RE ]] # t3
# grep RE file                          # t4
#
# Notes:
#
# Repeated reads of the same file probably utilizes
# Kernel cache to some extent. But is is still much faster
# to read file once and then apply matching.

FILE: t-file-newest-in-dir.sh
#! /bin/bash
#
# Q: What is the fastest way to get newest file in directory
# A: find + awk is tad faster but more complex. Use find + filters.
#
# t1 real    0m0.417s   find + awk
# t2 real    0m0.523s   find + sort + head + cut
# t3 real    0m0.575s   find + sort + sed
#
# t4 real    0m0.382s   stat (not a generic solution; see awk)
# t5 real    0m0.330s   ls -t (not a generic solution; see awk)
#
# Code:
#
# See <file>.sh for more details.
#
# find -maxdepth 1 -type f ... | awk '<complex code>'       # t1
# find -maxdepth 1 -type f | sort -r | head -1 | cut ...    # t2
# find -maxdepth 1 -type f | sort -r | sed ...              # t3
# stat ... | sort -r | sed ...                              # t4
# ls --sort=time | head -1                                  # t5
#
# Notes:
#
# awk(1) binary is smaller that sed(1)
#
# Probably small head(1) and cut(1) combined is still
# faster than sed(1) which uses regexp engine.
#
# These can't tell files from directories:
#
#   ls -t   sort by time
#   stat

FILE: t-file-pipe-vs-process-substitution.sh
#! /bin/bash
#
# Q: Would pipe be slower than using process substitution?
# A: No real difference. Pipes are efficient.
#
# t1 real    0m0.790s  pipes
# t2 real    0m0.745s  process substitution
#
# Code:
#
# cmd | cmd | cmd           # t1
# < <( < <(cmd) cmd) cmd    # t2

FILE: t-file-read-cat-vs-bash.sh
#! /bin/bash
#
# Q: Is Bash $(< FILE) faster than $(cat FILE)?
# A: The $(< FILE) is about 2x faster for small files
#
# real    0m0.166s $(< file)
# real    0m0.365s $(cat file)
#
# Notes:
#
# With big files, cat(1) is much faster:
#
# . t-lib.sh
# RandomWordsDictionary 1M > t.1M
#
# time bash -c 'cat t.1M > /dev/null'
# real    0m0.014s
#
# time bash -c 's=$(< t.1M); echo "$s" > /dev/null'
# real  0m0.055s

FILE: t-file-read-content-loop.sh
#! /bin/bash
#
# Q: Fastest way to process lines: 'while read < FILE' vs readarray
# A: readarray with 'for' loop is the fastest
#
# t1  real       0m0.037s t1  mapfile
# t2a real       0m0.033s t2a readarray + for
# t2b real       0m0.081s t2b readarray + for ((i++))  (!)
# t3  real       0m0.103s t3  while read < file
#
# Code:
#
#  mapfile -t array < file ; for <array> ...        # t1
#  readarray -t array < $f ; for i in <array> ...   # t2a
#  readarray -t array < $f ; for ((i... <array> ... # t2b
#  while read ... done < file                       # t3
#
# Notes:
#
# In Bash, the readarray built-in is a synonym for mapfile.

FILE: t-file-read-match-lines-loop-vs-grep.sh
#! /bin/bash
#
# Q: Will prefilter grep + loop help compared to straight loop?
# A: Yes, using external grep + loop is 2x faster
#
# t1a real    0m1.063s loop: case glob
# t1b real    0m1.059s loop: bash glob [[ ]]
# t2  real    0m0.424s grep before loop
#
# Code:
#
# while read ... done < file        # t1a
# while read ... done < file        # t1b
# grep | while ... done             # t2
#

FILE: t-file-read-shell-result.sh
#! /bin/bash
#
# Q: Is capturing var=$() faster or using temporary file for outpur?
# A: The var=$() is 2x faster than using a temporary file
#
# t1 real    0m0.428s val=$(cmd)
# t2 real    0m0.899s cmd > file; val=$(< file)

FILE: t-file-read-with-size-check.sh
#! /bin/bash
#
# Q: Is "test -s" for size useful before reading the file content?
# A: yes, much faster to check [ -s file ] before reading.
#
# real    0m0.105s $(< file)
# real    0m0.006s [ -s file] && $(< file)

FILE: t-file-size-info.sh
#! /bin/bash
#
# Q: What is the fastest way to read a file's size?
# A: GNU wc -c. Or stat() but it is not in POSIX (not portable)
#
# t1 real    0m0.288s stat -c file
# t2 real    0m0.360s wc -l file; GNU version efectively like stat()
# t3 real    0m0.461s ls -l + awk

FILE: t-lib.sh
FILE: t-loop-seq-vs-seq-expr.sh
#! /bin/bash
#
# Q: for-loop: Bash {1..N} vs $(seq N) vs POSIX i++
# A: The {1..N} and $(seq N) are very fast.
#
# t1 real    0m0.003s for i in {N..M}
# t2 real    0m0.007s for i in $(seq ...)
# t3 real    0m0.017s for [ $i -le $max ] ... i++

FILE: t-string-trim-whitespace.sh
#! /bin/bash
#
# Q: Trim whitepace using Bash RE vs sed(1)
# A: Bash is much faster; especially with fn() name ref
#
# t2 real    0m0.025s Bash fn() RE, name ref
# t2 real    0m0.107s Bash fn() RE
# t1 real    0m0.440s echo | sed RE
#
# Code:
#
# t1 var=$(echo .. | sed <trim>)    # external call
# t2 var=$(bashTrim)                # fn() return by value
# t2 BashTrom var                   # fn() use name ref

