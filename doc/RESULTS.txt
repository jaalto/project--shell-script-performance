FILE: t-command-grep.sh
#! /bin/bash
#
# Q: In grep, is --fixed-strings faster?
# A: Not much difference to --extended-regexp or --perl-regexp
#
# Q: Is using parallel(1) with grep even faster?
# A: Not worth for small files. Yes with bigger ones (test file: 10 000 lines)
#
# t1pure     real   0m0.338s LANG=C --fixed-strings
# t1utf8     real   0m0.372s LANG=C.UTF-8 --fixed-strings
# t1extended real   0m0.346s LANG=C --extended-regexp
# t1perl     real   0m0.349s LANG=C --perl-regexp
#
# t2icasef   real    0m0.394s LANG=C --fixed-strings --ignore-case
# t2icasee   real    0m0.419s LANG=C --extended-regexp --ignore-case
#
# GNU parallel(1). Split file into chunks and run grep(1) in parallel
# for each chunk.
#
# t_parallel1 real  0m0.226s <defaults>
# t_parallel2 real  0m0.653s --block-size 1k
# t_parallel3 real  0m0.300s -N 1k (grep instance for every 1k lines)

FILE: t-dir-empty.sh
#! /bin/bash
#
# Q: What is the fastest way to check empty directory?
# A: array+glob is faster than built-in compgen
#
# t1 real    0m0.054s   array+glob
# t2 real    0m0.104s   compgen
# t3 real    0m0.304s   ls (out of curiosity)
# t3 real    0m0.480s   find|read

FILE: t-dir-entries.sh
#! /bin/bash
#
# Q: Fastest to get list of dirs: for vs compgen vs ls -d
# A: In general, simple ls(1) will do fine. No big differences.
#
# for 20 directories:
#
# t3 real    0m0.003s compgen -G */
# t1 real    0m0.001s for-loop
# t2 real    0m0.004s ls -d */
#
# for 100 directories:
#
# t1 real    0m0.012s compgen -G */
# t2 real    0m0.015s for-loop
# t3 real    0m0.010s ls -d */
#
# Notes:
#
# Because the OS caches files and directories, you have to
# manually run tests:
#
#     max_dirs=20 ./t-dir-entries.sh t1
#     max_dirs=20 ./t-dir-entries.sh t2
#     max_dirs=20 ./t-dir-entries.sh t3

FILE: t-file-copy-check-exist.sh
#! /bin/bash
#
# Q: Need a copy of file. Call cp(1), make hardlink, or do test before copy?
# A: Faster is to test existense of file before cp(1). Hardlink is fast.
#
# t1 real    0m1.002s cp A B
# t2 real    0m0.013s [ A -nt B] && cp
# t2 real    0m0.009s [ A -ef B] || cp (using hardlink)
#
# Code:
#
# cp --preserve=timestamps A B                       # t1
# [ A -nt B ] || cp --preserve=timestamps ...        # t2
# [ A -ef B ] || cp --preserve=timestamps --link ... # t3

FILE: t-file-for-loop-vs-awk.sh
#! /bin/bash
#
# Q: for-loop file-by-file to awk vs awk handling all the files?
# A: Doing all in awk is at least 2x faster
#
# t1 real    0m0.213s awk '{...}' <file> <file> ...
# t1 real    0m0.584s for <files> do ... awk <file> ... done

FILE: t-file-glob-bash-compgen-vs-stat.sh
#! /bin/bash
#
# Q: The check if GLOB matches file: stat or Bash compgen?
# A: Bash array+glob/compgen are much faster than stat(1)
#
# t1 real    0m0.026s   Bash compgen GLOB
# t2 real    0m0.028s   Bash array: (GLOB)
# t2 real    0m0.039s   stat -t GLOB
#
# Code:
#
# arr=("file"*)
# compgen -G "file"*
# stat -t "file"*
#
# Notes:
#
# stat(1) does more work by opening each found file.

FILE: t-file-grep-vs-match-in-memory.sh
#! /bin/bash
#
# Q: To check file for matches: repeat read, inline match or grep(1)?
# A: Fastest is to read file once into memory and then match
#
# t1a real   0m0.049s read + bash regexp (read file once + use loop)
# t1b real   0m0.117s read + case..MATCH..esac (read file once + use loop)
# t2  real   0m0.482s read + case..MATCH..esac (separate file calls)
# t3  real   0m0.448s read + bash regexp (separate file calls)
# t4  real   0m0.404s external grep(1)
#
# Code:
#
# Study the <file>.sh for more information.
#
# read once and loop [[ str =~~ RE ]]   # t1a
# read once and loop case..MATCH..end   # t1b
# read -N<max> < file. case..MATCH..end # t2
# read -N<max> < file. [[ str =~~ RE ]] # t3
# grep RE file                          # t4
#
# Notes:
#
# Repeated reads of the same file probably utilizes
# Kernel cache to some extent. But is is still much faster
# to read file once and then apply matching.

FILE: t-file-newest-in-dir.sh
#! /bin/bash
#
# Q: What is the fastest way to get newest file in directory
# A: find + awk is tad faster but more complex. Use find + filters.
#
# t1 real    0m0.417s   find + awk
# t2 real    0m0.523s   find + sort + head + cut
# t3 real    0m0.575s   find + sort + sed
#
# t4 real    0m0.382s   stat (not a generic solution)
# t5 real    0m0.330s   ls -t (not a generic solution)
#
# Code:
#
# See <file>.sh for more details.
#
# find -maxdepth 1 -type f ... | awk '<complex code>'       # t1
# find -maxdepth 1 -type f | sort -r | head -1 | cut ...    # t2
# find -maxdepth 1 -type f | sort -r | sed ...              # t3
# stat ... | sort -r | sed ...                              # t4
# ls --sort=time | head -1                                  # t5
#
# Notes:
#
# awk(1) binary is smaller that sed(1)
#
# Probably small head(1) and cut(1) combined is still
# faster than sed(1) which uses regexp engine.
#
# These can't tell files from directories:
#
#   ls -t   sort by time
#   stat

FILE: t-file-pipe-vs-process-substitution.sh
#! /bin/bash
#
# Q: Would pipe be slower than using process substitution?
# A: No real difference. Pipes are efficient.
#
# t1 real    0m0.790s  pipes
# t2 real    0m0.745s  process substitution
#
# Code:
#
# cmd | cmd | cmd           # t1
# < <( < <(cmd) cmd) cmd    # t2

FILE: t-file-read-cat-vs-bash.sh
#! /bin/bash
#
# Q: Is Bash $(< FILE) faster than $(cat FILE)?
# A: The $(< FILE) is about 2x faster for small files
#
# real    0m0.166s $(< file)
# real    0m0.365s $(cat file)
#
# Notes:
#
# With big files, they are equal.
#
# . ./t-lib.sh; RandomWordsDictionary 1M > t.1M
#
# time bash -c 's=$(cat t.1M); echo "$s" > /dev/null'
# real    0m0.059s
#
# time bash -c 's=$(< t.1M); echo "$s" > /dev/null'
# real  0m0.056s

FILE: t-file-read-content-loop.sh
#! /bin/bash
#
# Q: Fastest to process lines: readarray vs 'while read < file' ?
# A: readarray/mapfiles+for is 2x faster than 'while read < file'
#
# t1  real       0m0.037s t1  mapfile + for
# t2a real       0m0.036s t2a readarray + for
# t2b real       0m0.081s t2b readarray + for ((i++))
# t3  real       0m0.085s t3  while read < file
#
# Code:
#
#  mapfile -t array < file   ; for <array> ...        # t1
#  readarray -t array < file ; for i in <array> ...   # t2a
#  readarray -t array < file ; for ((i... <array> ... # t2b
#  while read ... done < file                         # t3
#
# Notes:
#
# In Bash, the readarray built-in is a synonym for mapfile,
# so they should behave equally.

FILE: t-file-read-match-lines-loop-vs-grep.sh
#! /bin/bash
#
# Q: process file: will prefilter lines using grep(1) help?
# A: grep(1) + loop is 2x faster than doing filtering in loop
#
# t1a real    0m0.436s grep prefilter before loop
# t1b real    0m0.469s grep prefilter before loop (proc)
# t2a real    0m1.105s loop: POSIX glob match with case...esac
# t2b real    0m1.127s loop: Bash glob match using [[ ]]
#
# Code:
#
# grep | while ... done                      # t1a
# while ... done < <(grep)                   # t1b
# while read ... case..esac ... done < file  # t2a
# while read ... [[ ]] ... done < file       # t2b
#
# Notes:
#
# The practical winner in scripts is the `while read
# do .. done < <(proc)` due to variables being
# visible in the same scope. The "grep | while"
# would create a subshell and release the variables
# after the for-loop.
#
# About the test cases
#
# The file contents read during the test cases are
# probably cached in the Kernel. When the tests are
# executed in the order "t1a t1b," reversing the
# order to "t1b t1a" results in the FIRST test
# consistently appearing to run faster. This is
# likely not an accurate representation of the true
# performance. The apparent equality in performance
# between cases "t1a" and "t2b" is probably due to
# the Kernel's file cache.

FILE: t-file-read-shell-result.sh
#! /bin/bash
#
# Q: Command result: var=$() vs reading output from a temporary file?
# A: The var=$() is 2x faster than using a temporary file
#
# t1 real    0m0.428s val=$(cmd)
# t2 real    0m0.899s cmd > file; val=$(< file)

FILE: t-file-read-with-size-check.sh
#! /bin/bash
#
# Q: Is empty file check useful before reading file's content?
# A: It is significantly faster (~10x) to use [ -s file ] before reading
#
# real    0m0.105s $(< file)
# real    0m0.006s [ -s file] && $(< file)

FILE: t-file-size-info.sh
#! /bin/bash
#
# Q: What is the fastest way to read a file's size?
# A: GNU 'wc -c'. The stat(1) is not in POSIX + options are not portable.
#
# t1 real    0m0.288s stat -c file
# t2 real    0m0.380s wc -c file; GNU version efectively is like stat(1)
# t3 real    0m0.461s ls -l + awk

FILE: t-function-return-value.sh
#! /bin/bash
#
# Q: Bash name ref to return values vs val=$(funcall)
# A: Using name ref is about 40x faster
#
# t1 real    0m0.089s t1 $(funcall)
# t2 real    0m0.002s t2 funcall nameref
#
# Code:
#
# fn(): ... echo "<value>"                 # t1
# fn(): local -n ret=$1; ... ret="<value>" # t2

FILE: t-lib.sh
FILE: t-statement-arithmetic-for-loop.sh
#! /bin/bash
#
# Q: for-loop: ((...)) vs {1..N} vs $(seq N) vs POSIX i++
# A: The {1..N} and $(seq N) are very fast.
#
# t1 real    0m0.003s for i in {1..N}
# t2 real    0m0.004s for i in $(seq ...)
# t3 real    0m0.006s for ((i=0; i < N; i++))
# t4 real    0m0.010s while [ $i -le $N ] ... i++
#
# Notes:
#
# A simple, elegant, and practical winner: $(seq N)
#
# {1..N} problem: The Bash brace
# expansion cannot be parameterized, so it
# is only useful if N is known beforehand.
#
# However, all loops are so fast that the
# numbers don't mean much. The POSIX while-loop
# variant was slightly slower in all subsequent
# tests.

FILE: t-statement-arithmetic-increment.sh
#! /bin/bash
#
# Q: POSIX `i=$((i + 1))` vs `((i++))` vs `let i++` etc.
# A: No noticeable difference, POSIX Â´i=$((i + 1))` will do fine
#
# t1     real    0m0.025s ((i++))      Bash
# t2     real    0m0.047s let i++      Bash
# t3     real    0m0.045s i=$((i + 1)) POSIX
# t4     real    0m0.061s : $((i++))   POSIX (side effect)
#
# Notes:
#
# The tests were using 10,000 repeats, which is
# unrealistic for any program. There really is no
# practical difference whichever you choose. The
# portable POSIX version works in all shells:
# `i=$((i + 1))`.

FILE: t-statement-if-test-posix-vs-bash.sh
#! /bin/bash
#
# Q: POSIX [ $var = 1 ] vs Bash [[ $var = 1 ]] etc
# A: In practise, not much difference
#
# t1val     real    0m0.002s [ "$var" = "1" ]
# t2val     real    0m0.003s [[ $var = 1 ]]
#
# t1empty   real    0m0.002s [ ! "$var" ]     # modern
# t2empty   real    0m0.002s [ -z "$var" ]    # archaic
# t3empty   real    0m0.003s [[ ! $var ]]     # Bash
#
# Notes:
#
# Only with very high amount of repeats, there are
# slight differences in favor of Bash `[[ ]]`.
#
# loop_max=10000 ./statement-if-posix-vs-bash.sh
#
# t1val          real 0.055  user 0.054  sys 0.000
# t2val          real 0.032  user 0.030  sys 0.003  [[ ]]
#
# t1empty        real 0.052  user 0.045  sys 0.007
# t2empty        real 0.053  user 0.050  sys 0.003
# t3empty        real 0.032  user 0.026  sys 0.007  [[ ]]

FILE: t-string-trim-whitespace.sh
#! /bin/bash
#
# Q: Trim whitepace using Bash RE vs sed(1)
# A: Bash is much faster; especially with fn() name ref
#
# t2 real    0m0.025s Bash fn() RE, name ref
# t2 real    0m0.107s Bash fn() RE
# t1 real    0m0.440s echo | sed RE
#
# Code:
#
# t1 var=$(echo .. | sed <trim>)    # external call
# t2 var=$(bashTrim)                # fn() return by value
# t2 BashTrim var                   # fn() use name ref

FILE: t-variable-array-split-string.sh
#! /bin/bash
#
# Q: split string into an array: read vs eval?
# A: eval is 2x faster
#
# t1 real    0m0.012s eval
# t2 real    0m0.025s read -ra
#
# Code:
#
# string=$(echo {1..100})
# read -ra array <<< "$string"  # t1
# eval 'array=($string)'        # t2
#
# Notes:
#
# The reason is probably that `<<<` uses a
# temporary file, whereas `eval` operates
# entirely in memory.

